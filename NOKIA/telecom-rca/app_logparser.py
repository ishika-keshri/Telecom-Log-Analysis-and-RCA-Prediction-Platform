import flask
import numpy as np
import pandas as pd
import joblib
import re
from scipy.sparse import issparse
# import sklearn
# print(f"Using scikit-learn version: {sklearn.__version__}") 

import os
import json # For converting dict to JSON string for the prompt
from google import genai
from pydantic import BaseModel

from flask_cors import CORS



class Insight(BaseModel):
    anomaly_probability: float
    normal_probability: float
    is_anomaly: int
    matched_event_id: str
    matched_event_template: str
    parsed_content: str
    log_line: str
    explanation: str # This will be generated by Gemini


client = genai.Client(api_key="AIzaSyDGrUHQSSeVt2YtD81SfAd9OjJkzOdg4-I")


app = flask.Flask(__name__)
CORS(app)

MODEL_PATH = './tuned_gb_model.pkl'
SCALER_PATH = './scaler.pkl'
PCA_PATH = './pca.pkl'
TEMPLATES_PATH = './parsed_logs/HDFS.log_templates.csv'
FEATURE_EXTRACTOR_PATH = './feature_extractor.pkl'

N_COMPONENTS_PCA = 94 

try:
    print("Attempting to load models and transformers...")
    model = joblib.load(MODEL_PATH)
    print(f"'{MODEL_PATH}' loaded.")
    scaler = joblib.load(SCALER_PATH)
    print(f"'{SCALER_PATH}' loaded.")
    pca = joblib.load(PCA_PATH)
    print(f"'{PCA_PATH}' loaded.")
    log_templates_df = pd.read_csv(TEMPLATES_PATH)
    print(f"'{TEMPLATES_PATH}' loaded.")
    feature_extractor = joblib.load(FEATURE_EXTRACTOR_PATH)
    print(f"'{FEATURE_EXTRACTOR_PATH}' loaded.")
    print("All models and transformers loaded successfully. ✅")
except FileNotFoundError as e:
    print(f"❌ Error loading model/scaler/pca/templates/feature_extractor: {e}")
    print("Please ensure all .pkl and .csv files are in the specified paths relative to app.py.")
    model, scaler, pca, log_templates_df, feature_extractor = None, None, None, None, None
except Exception as e:
    print(f"❌ An unexpected error occurred during loading: {e}")
    import traceback
    traceback.print_exc()
    model, scaler, pca, log_templates_df, feature_extractor = None, None, None, None, None


# Regex for parsing HDFS log line
LOG_FORMAT_REGEX = re.compile(r'^\d{6}\s+\d{6}\s+\d+\s+\w+\s+[\w\.\$-]+:\s+(.*)$')


# --- Helper Functions ---

def parse_log_line_to_content(log_line):
    match = LOG_FORMAT_REGEX.match(log_line.strip())
    if match:
        content = match.group(1)
        return content
    print(f"Log line did not match HDFS format: {log_line}")
    return None

def get_event_id_from_content(content, templates_df):
    if templates_df is None:
        print("Log templates DataFrame not loaded (is None).")
        return None
    if templates_df.empty:
        print("Log templates DataFrame is empty.")
        return None

    for _, row in templates_df.iterrows():
        template = str(row['EventTemplate']) 
        regex_pattern = re.escape(template)
        regex_pattern = regex_pattern.replace(re.escape("<*>"), "(.*?)")
        # Add other specific wildcard replacements if your IPLoM setup uses them
        # e.g., regex_pattern = regex_pattern.replace(re.escape("blk_<*>"), r"blk_[-0-9a-zA-Z]+")
        regex_pattern = f"^{regex_pattern}$"
        try:
            if re.match(regex_pattern, content):
                return row['EventId']
        except re.error as e:
            print(f"Regex error for template '{template}': {e}")
            continue
    print(f"Content did not match any template: '{content}'")
    return None

# --- Flask Routes ---
@app.route('/')
def home():
    return "HDFS Log Anomaly Detection Server. Send a POST request to /predict with 'log_line' in JSON."

@app.route('/predict', methods=['POST'])
def predict_anomaly():
    # Check if all components are loaded
    if any(x is None for x in [model, scaler, pca, log_templates_df, feature_extractor]):
        print("Error during prediction: One or more critical components (model, scaler, etc.) are None.")
        return flask.jsonify({'error': 'Models or helper data not loaded properly on the server. Check startup logs.'}), 500

    data = flask.request.get_json()
    if not data or 'log_line' not in data:
        return flask.jsonify({'error': 'Missing "log_line" in request JSON data.'}), 400

    raw_log_line = data['log_line']
    print(f"\nReceived log line for prediction: {raw_log_line}")

    # 1. Parse log line to get content
    log_content = parse_log_line_to_content(raw_log_line)
    if log_content is None:
        return flask.jsonify({
            'error': 'Invalid log line format. Could not parse content.',
            'log_line': raw_log_line
        }), 400
    print(f"Parsed content: {log_content}")

    # 2. Match content to an EventId from templates
    event_id = get_event_id_from_content(log_content, log_templates_df)
    print(f"Matched EventId: {event_id}")
    
    matched_event_template = "No Match"
    if event_id:
        template_row = log_templates_df[log_templates_df['EventId'] == event_id]
        if not template_row.empty:
            matched_event_template = template_row.iloc[0]['EventTemplate']

    if event_id is None:
        print("No EventId matched. Using '<UNK>' token for feature extraction.")
        log_sequence_for_fe = [['<UNK>']] 
    else:
        log_sequence_for_fe = [[str(event_id)]]
    print(f"Sequence for FeatureExtractor (as list): {log_sequence_for_fe}")

    # 3. Create feature vector using the loaded FeatureExtractor
    try:
        n_expected_features = scaler.n_features_in_
    except AttributeError:
        print("Error: Loaded scaler does not have 'n_features_in_'. Was it fitted correctly?")
        return flask.jsonify({'error': 'Internal server error: Scaler not properly initialized.'}), 500
    print(f"Scaler expects {n_expected_features} features.")

    feature_vec = np.zeros((1, n_expected_features))
    print(f"Initialized feature_vec to zeros with shape: {feature_vec.shape}")

    try:
        np_input_to_transform = np.array(log_sequence_for_fe)
        print(f"Converted sequence to NumPy array for transform. Shape: {np_input_to_transform.shape}, Type: {type(np_input_to_transform)}")

        transformed_output = feature_extractor.transform(np_input_to_transform)
        print(f"FeatureExtractor output type: {type(transformed_output)}, raw output: {transformed_output}")


        # Process transformed_output (expected: sparse matrix or ndarray)
        processed_output_np = None
        if issparse(transformed_output):
            processed_output_np = transformed_output.toarray()
            print(f"Converted sparse matrix to dense. Shape: {processed_output_np.shape if processed_output_np is not None else 'None'}")
        elif isinstance(transformed_output, np.ndarray):
            processed_output_np = transformed_output
            print(f"FeatureExtractor output is ndarray. Shape: {processed_output_np.shape if processed_output_np is not None else 'None'}")
        else:
            print(f"Warning: FeatureExtractor.transform returned unhandled type: {type(transformed_output)}. Using default zero vector for feature_vec.")
            # feature_vec remains the initialized zero vector in this case

        if processed_output_np is not None:
            # Validate and reshape if necessary
            if processed_output_np.ndim == 1:
                processed_output_np = processed_output_np.reshape(1, -1)
            
            # Check if the shape is as expected for assignment
            if processed_output_np.shape == (1, n_expected_features):
                feature_vec = processed_output_np  # Assign the valid, processed output
            elif processed_output_np.shape[0] == 1 and processed_output_np.shape[1] == 0 and n_expected_features > 0 :
                # This can happen if feature_extractor can't find the token (e.g. <UNK>) and returns (1,0)
                print(f"FeatureExtractor returned shape {processed_output_np.shape}. Using default zero vector.")
                # feature_vec remains the initialized zero vector
            else:
                print(f"Shape mismatch from transform: got {processed_output_np.shape}, expected (1, {n_expected_features}). Using default zero vector.")
                # feature_vec remains the initialized zero vector
        
        print(f"Final feature_vec for scaling. Shape: {feature_vec.shape}")

    except Exception as e: # Catch any error during transform or processing
        print(f"Error during feature vector creation/validation: {str(e)}")
        import traceback
        traceback.print_exc()
        # feature_vec is already the initialized default zero vector.
        # Decide if you want to proceed with zeros or return an error.
        # For now, returning an error to highlight the issue:
        return flask.jsonify({'error': f'Failed during feature vector processing: {str(e)}'}), 500
    
    # 4. Standardize and Apply PCA
    # feature_vec is now guaranteed to be a 2D NumPy array with shape (1, n_expected_features)
    try:
        scaled_features = scaler.transform(feature_vec)
        print(f"Scaled features shape: {scaled_features.shape}")
        
        pca_output = pca.transform(scaled_features)
        print(f"PCA output shape: {pca_output.shape}")
        if pca_output.shape[1] < N_COMPONENTS_PCA:
            print(f"Error: PCA output has {pca_output.shape[1]} components, expected at least {N_COMPONENTS_PCA}.")
            return flask.jsonify({'error': f'PCA output has {pca_output.shape[1]} components, expected at least {N_COMPONENTS_PCA}.'}), 500
        
        pca_features_selected = pca_output[:, :N_COMPONENTS_PCA]
        print(f"Selected PCA features shape: {pca_features_selected.shape}")

    except Exception as e:
        print(f"Error during feature transformation (scaling/PCA): {e}")
        import traceback
        traceback.print_exc()
        return flask.jsonify({'error': f'Error during feature transformation: {str(e)}'}), 500

    # 5. Predict
    try:
        prediction_val = model.predict(pca_features_selected)
        probabilities = model.predict_proba(pca_features_selected)
        print(f"Prediction: {prediction_val[0]}, Probabilities: {probabilities[0]}")

        result = {
            'log_line': raw_log_line,
            'parsed_content': log_content,
            'matched_event_id': event_id if event_id else "No Match",
            'matched_event_template': matched_event_template,
            'is_anomaly': int(prediction_val[0]),
            'anomaly_probability': float(probabilities[0][1]),
            'normal_probability': float(probabilities[0][0])
        }

        input_data = str(result)
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=f"""
        You are an insights generator. Given the following anomaly record in JSON:
        {input_data}
        Produce a JSON object that includes all original fields plus an "explanation" field. The explanation should describe in clear, human-understandable language what happened, why this record is anomalous, and any relevant context.
        """,
            config={
                "response_mime_type": "application/json",
                "response_schema": Insight,
            },
        )
        insight: Insight = response.parsed
        expl = insight.explanation 
        result = {
            'log_line': raw_log_line,
            'parsed_content': log_content,
            'matched_event_id': event_id if event_id else "No Match",
            'matched_event_template': matched_event_template,
            'is_anomaly': int(prediction_val[0]),
            'anomaly_probability': float(probabilities[0][1]),
            'normal_probability': float(probabilities[0][0]), 
            'explanation' : expl , 
        }
        print(result)
        return flask.jsonify(result)
    except Exception as e:
        print(f"Error during prediction: {e}")
        import traceback
        traceback.print_exc()
        return flask.jsonify({'error': f'Error during prediction: {str(e)}'}), 500


if __name__ == '__main__':
    components_loaded = True
    if model is None: print("❌ Error: 'model' failed to load."); components_loaded = False
    if scaler is None: print("❌ Error: 'scaler' failed to load."); components_loaded = False
    if pca is None: print("❌ Error: 'pca' failed to load."); components_loaded = False
    if log_templates_df is None: print("❌ Error: 'log_templates_df' failed to load."); components_loaded = False
    if feature_extractor is None: print("❌ Error: 'feature_extractor' failed to load."); components_loaded = False

    if not components_loaded:
        print("Server cannot start due to missing or failed model/data files. "
              "Please check error messages above and ensure all required files are present and loadable.")
    else:
        print(f"All models and data loaded. Flask server starting on port 5000. PCA components to use: {N_COMPONENTS_PCA}")
        app.run(host='0.0.0.0', port=5000, debug=False) # Set debug=True for more error details during development
